{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core concept:\n",
    "- `tensor.view()` creates a new view of the same underlying data but with a different shape\n",
    "- Memory-efficient and no new data is allocated.\n",
    "- Shared memory: Because the original tensor and its view share the same memory, any in-place changes to the view will also modify the original tensor, and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x  = torch.arange(12)\n",
    "print(x.is_contiguous())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.view()` works only on <b>CONTIGUOUS TENSORS</b>\n",
    "- If the tensor is not contiguous, `.view()` will throw an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3)\n",
    "y = x.t()\n",
    "print(y.is_contiguous())  # False\n",
    "y.view(6)                 # RuntimeError\n",
    "\n",
    "\n",
    "y = y.contiguous().view(6)  # ✅ Works now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTIGUITY in TENSORS\n",
    "- Logical order of the tensor is the same as the physical order in memory\n",
    "Eg. ` a = [[1,2,3],[4,5,6]]\n",
    "- In memory, it looks like : [1,2,3,4,5,6]\n",
    "- stride = `(s_x,s_y)` = `(3,1)` (number of places moved in memory to reach the next row element,  number of places moved in memory to reach the next col element)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For a tensor with shape (s_0, s_1, ..., s_n-1), the strides (t_0, t_1, ..., t_n-1) must satisfy the following: Innermost dimension stride: \n",
    "- The stride of the last dimension (\\(t_{n-1}\\)) must be 1. \n",
    "- This means adjacent elements in the final dimension are also adjacent in memory.\n",
    "- General stride condition: For any other dimension i, the stride t_i must be equal to the product of the sizes of all dimensions to its right.\n",
    "\n",
    "Eg Consider a tensor x with shape (4, 3, 2). \n",
    "- Its expected contiguous strides would be (6, 2, 1).\n",
    "- `t_2 = 1 `;  `t_1 = t_2 * s_1` ; `t_0 = t_1 * s_0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations that destroy CONTIGUITY:\n",
    "- `.permute`\n",
    "- `.transpose` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "- - - - - -- - - - - -- - - -- - - -\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Row/Col matrices are contiguous even when transposed\n",
    "\n",
    "y = torch.rand((1,6))\n",
    "print(y.is_contiguous())\n",
    "print(y.T.is_contiguous())\n",
    "print(y.reshape(2,3).is_contiguous())\n",
    "\n",
    "print(\"- - - - - -- - - - - -- - - -- - - -\")\n",
    "x = torch.tensor(([1,2,3],[4,5,6]))\n",
    "print(x.is_contiguous())\n",
    "print(x.T.is_contiguous())\n",
    "print(x.reshape(3,2).is_contiguous())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `-1` magic dimension\n",
    "- In .view() you can enter -1 to let PyTorch infer the dimension of the reshaped tensor automatically \n",
    "\n",
    "- PyTorch will compute the missing dimension accordingly whichever position `-1` is in\n",
    "\n",
    "- Only one -1 is allowed.\n",
    "\n",
    "- The total number of elements must match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(12)\n",
    "b = a.view(3,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WHen to use `.view()` vs `.reshape()`\n",
    "- When you know that the tensor is contiguous, then use `.view()`\n",
    "- If not sure, use `.reshape()`\n",
    "\n",
    "But `.reshape()` is less memory efficient than `.view()` because it creates a new tensor in memory, although its not guaranteed to do so\n",
    "- `.reshape()` creates a copy and allocates new memory when the original tensor is not contiguous. A tensor becomes non-contiguous after certain operations, like `.permute()` or `.transpose()`, which reorder the dimensions without changing the underlying data layout. \n",
    "\n",
    "- If the input tensor is already contiguous, reshape() will simply return a view of the original tensor, just like view() does. In this case, no new memory is allocated, and the operation is just as memory-efficient as view()\n",
    "\n",
    "- `.view()` is more restrictive and will only return a view if the tensor is contiguous. If the tensor is non-contiguous, calling `.view()` will result in a RuntimeError. This forces the user to explicitly handle the contiguity, typically by calling `.contiguous()` first, which performs the memory copy manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "tensor([[0.0801, 0.7271, 0.3827],\n",
      "        [0.2099, 0.8101, 0.2096]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((2,3))\n",
    "print(a.is_contiguous())\n",
    "b = a.T\n",
    "print(b.is_contiguous())\n",
    "\n",
    "\n",
    "a.view(3,-1) # Will work because the tensor is contiguos\n",
    "b.view(2,-1) # runtime error because b is not contiguous\n",
    "\n",
    "print(b.reshape(2,-1)) # Will work even when the tensor is not contiguous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between `.view()` and `.flatten()`\n",
    "\n",
    "` x.view(-1) == x.flatten() `\n",
    "\n",
    "The only difference:\n",
    "- `.flatten()` can flatten across specific dimensions (start_dim, end_dim)\n",
    "- `.view(-1)` flattens the entire tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 210])\n",
      "torch.Size([2520])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((3,4,5,6,7)) # shape = (3,4,5,6,7)\n",
    "b = a.flatten(start_dim=2) # shape = (3,4,210)\n",
    "print(b.shape) # Flattens starting from the start_dim\n",
    "\n",
    "c = a.view(-1) # flattens along all dimensions\n",
    "print(c.shape) # [2520,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When .view() fails silently (common trap)\n",
    "- You may think .view() reshaped the tensor correctly, but if the tensor was non-contiguous, and you used .contiguous() incorrectly, you might accidentally copy data unknowingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "y = x.permute(0, 2, 1)          # non-contiguous\n",
    "z = y.contiguous().view(2, -1)  # makes a copy\n",
    "\n",
    "# Z no longer shares memory with y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.view_as() `\n",
    "- `x.view_as(y) gives x the same shape as y`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((2,3,4))\n",
    "a.view(3,2,4)\n",
    "b = torch.zeros((2,4,3)).view_as(a)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, `.view()` is most used in reshaping between:\n",
    "\n",
    "- CNN outputs and fully connected layers.\n",
    "- Flattening (N, C, H, W) → (N, C*H*W) before Linear layers.\n",
    "- Expanding dimensions for broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(32, 512, 7, 7)\n",
    "x = x.view(32, -1)  # shape: (32, 512*7*7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can use `.view() ` to add or align dimensions for broadcasting.\n",
    "- `.view()` doesn't create a new tensor in memory and is memory-efficient making it the first choice in CNN flattening layers\n",
    "- In-place computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(32, 100)\n",
    "bias = torch.randn(100)\n",
    "x = x + bias.view(1, 100)  # (32, 100) + (1, 100) ✅ broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n",
      "torch.Size([1, 100])\n",
      "torch.Size([32, 100])\n"
     ]
    }
   ],
   "source": [
    "print(bias.shape)\n",
    "bias = bias.view(1, -1)\n",
    "print(bias.shape)\n",
    "bias = bias.expand(32, -1)\n",
    "print(bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another time when  `.view()` fails is when we take a transpose of the tensor. \n",
    "- The reason is because the stride of the tensor changes. \n",
    "- The stride is n-dim tuple where each element gives the number of elements to move in memory to reach the next element in that dimension\n",
    "- When the transpose is taken, the stride changes and makes the tensor non-contiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 4, 1)\n",
      "False\n",
      "(1, 4, 12)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((2,3,4))\n",
    "print(x.stride())\n",
    "y  = x.T\n",
    "print(y.is_contiguous())\n",
    "print(y.stride())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## “A tensor can only be contiguous if stride[-1] == 1.”\n",
    "'''\n",
    "In contiguous memory layout, elements along the last dimension are stored next to each other.\n",
    "That means their stride (the step to move by one element in memory) must be 1.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn contiguous memory layout, elements along the last dimension are stored next to each other.\\nThat means their stride (the step to move by one element in memory) must be 1.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "You have a tensor x = torch.arange(12)\n",
    "You call x.view(3, 4) and x.view(2, 6).\n",
    "Both are valid — why? What is the general mathematical rule that decides whether a .view() operation is valid?\n",
    "'''\n",
    "\n",
    "'''\n",
    "The number of elements in the tensor is 12. And they are stored in contiguous memory.And the stride is (1). \n",
    "When the shape is changed to 3,4 or 2,6 this first dimension of the stride is not changed and the number of elements is still the same. \n",
    "Therefore both are valid ops\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3)\n",
    "x = x.t()\n",
    "x.view(6)\n",
    "\n",
    "## here what do you mean by x is not-contiguous?\n",
    "\n",
    "'''\n",
    "After transposing the stride of x becomes (1,3) and this makes the tensor not contiguous\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID of Python object `a`: 0x121013ef0\n",
      "ID of Python object `b`: 0x12157b350\n",
      "---\n",
      "Data pointer of tensor `a`: 0x1368d9c80\n",
      "Data pointer of tensor `b`: 0x1368d9c80\n",
      "\n",
      "Do a and b share the same data? True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" \\nYes the original tensor will change because .view() is an in-place operation and doesn't create a copy unlike .reshape() \\nCorrupted gradients: If you make an in-place modification on a tensor that is a view, you are also changing the original tensor, which may have been saved for a gradient calculation. When the backward() pass runs, it will use the modified data instead of the original data, leading to incorrect or unpredictable gradients.\\nFailed checks: To prevent this, PyTorch often performs checks during backpropagation. If it detects that a tensor has been modified in-place, it will raise a RuntimeError to alert you of the invalid state. \\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## If you modify a tensor created via .view(), will the original tensor change?\n",
    "## Why might this be dangerous in backpropagation or data preprocessing pipelines?\n",
    "\n",
    "import torch\n",
    "\n",
    "# Create the initial tensor\n",
    "a = torch.randn((2, 4, 5))\n",
    "\n",
    "# Create a view of the tensor\n",
    "b = a.view(2, -1)\n",
    "\n",
    "# Get the memory address of the Python objects (will be different)\n",
    "mem_a_obj = id(a)\n",
    "mem_b_obj = id(b)\n",
    "\n",
    "# Get the memory address of the underlying data (will be the same)\n",
    "mem_a_data = a.data_ptr()\n",
    "mem_b_data = b.data_ptr()\n",
    "\n",
    "print(f\"ID of Python object `a`: {hex(mem_a_obj)}\")\n",
    "print(f\"ID of Python object `b`: {hex(mem_b_obj)}\")\n",
    "print(\"---\")\n",
    "print(f\"Data pointer of tensor `a`: {hex(mem_a_data)}\")\n",
    "print(f\"Data pointer of tensor `b`: {hex(mem_b_data)}\")\n",
    "\n",
    "# Check if the data pointers are equal\n",
    "print(f\"\\nDo a and b share the same data? {a.data_ptr() == b.data_ptr()}\")\n",
    "\n",
    "''' \n",
    "Yes the original tensor will change because .view() is an in-place operation and doesn't create a copy unlike .reshape() \n",
    "Corrupted gradients: If you make an in-place modification on a tensor that is a view, you are also changing the original tensor, which may have been saved for a gradient calculation. When the backward() pass runs, it will use the modified data instead of the original data, leading to incorrect or unpredictable gradients.\n",
    "Failed checks: To prevent this, PyTorch often performs checks during backpropagation. If it detects that a tensor has been modified in-place, it will raise a RuntimeError to alert you of the invalid state. \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How `.contiguous()` fixes it\n",
    "- The `.contiguous()` method fixes the contiguity problem by forcing a memory copy. \n",
    "- It allocates a new, fresh block of memory and copies the data from the non-contiguous tensor into it, arranged in the correct, sequential (row-major) order for its new shape. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### The problem: Sparse matrix compression\n",
    "- You are given a 2D tensor M representing a sparse matrix. The matrix contains many zero values. To save memory and processing time, you want to compress this matrix into a 1D tensor C that only stores the non-zero values.\n",
    "- The non-zero values should be arranged sequentially in C, preserving their original row-major order.\n",
    "- The challenge is to perform this compression using only zero-copy operations (view(), permute(), narrow(), etc.) and basic tensor indexing. If a memory copy is necessary at any point, the solution is considered incorrect.\n",
    "- You must then decompress the 1D tensor C back into a 2D tensor of the original matrix's shape. This decompression must also be a zero-copy operation.\n",
    "- Finally, write a function is_valid_compression(M, C_decompressed) that checks if the decompression successfully reproduced the original non-zero values and their positions, without using any loops.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Constraints:\n",
    "1. The original matrix M is a 2D tensor of shape (R, K).\n",
    "2. You cannot use torch.reshape() on non-contiguous tensors.\n",
    "3. The use of torch.flatten() or .contiguous() is strictly forbidden for both compression and decompression.\n",
    "4. The final tensors for decompression and validation must be views of the original tensor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = torch.rand((10,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mentoroid_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
