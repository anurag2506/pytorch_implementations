{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core concept:\n",
    "- `tensor.view()` creates a new view of the same underlying data but with a different shape\n",
    "- Memory-efficient and no new data is allocated.\n",
    "- Shared memory: Because the original tensor and its view share the same memory, any in-place changes to the view will also modify the original tensor, and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x  = torch.arange(12)\n",
    "print(x.is_contiguous())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.view()` works only on <b>CONTIGUOUS TENSORS</b>\n",
    "- If the tensor is not contiguous, `.view()` will throw an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3)\n",
    "y = x.t()\n",
    "print(y.is_contiguous())  # False\n",
    "y.view(6)                 # RuntimeError\n",
    "\n",
    "\n",
    "y = y.contiguous().view(6)  # ✅ Works now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTIGUITY in TENSORS\n",
    "- Logical order of the tensor is the same as the physical order in memory\n",
    "Eg. ` a = [[1,2,3],[4,5,6]]\n",
    "- In memory, it looks like : [1,2,3,4,5,6]\n",
    "- stride = `(s_x,s_y)` = `(3,1)` (number of places moved in memory to reach the next row element,  number of places moved in memory to reach the next col element)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For a tensor with shape (s_0, s_1, ..., s_n-1), the strides (t_0, t_1, ..., t_n-1) must satisfy the following: Innermost dimension stride: \n",
    "- The stride of the last dimension (\\(t_{n-1}\\)) must be 1. \n",
    "- This means adjacent elements in the final dimension are also adjacent in memory.\n",
    "- General stride condition: For any other dimension i, the stride t_i must be equal to the product of the sizes of all dimensions to its right.\n",
    "\n",
    "Eg Consider a tensor x with shape (4, 3, 2). \n",
    "- Its expected contiguous strides would be (6, 2, 1).\n",
    "- `t_2 = 1 `;  `t_1 = t_2 * s_1` ; `t_0 = t_1 * s_0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations that destroy CONTIGUITY:\n",
    "- `.permute`\n",
    "- `.transpose` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "- - - - - -- - - - - -- - - -- - - -\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Row/Col matrices are contiguous even when transposed\n",
    "\n",
    "y = torch.rand((1,6))\n",
    "print(y.is_contiguous())\n",
    "print(y.T.is_contiguous())\n",
    "print(y.reshape(2,3).is_contiguous())\n",
    "\n",
    "print(\"- - - - - -- - - - - -- - - -- - - -\")\n",
    "x = torch.tensor(([1,2,3],[4,5,6]))\n",
    "print(x.is_contiguous())\n",
    "print(x.T.is_contiguous())\n",
    "print(x.reshape(3,2).is_contiguous())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `-1` magic dimension\n",
    "- In .view() you can enter -1 to let PyTorch infer the dimension of the reshaped tensor automatically \n",
    "\n",
    "- PyTorch will compute the missing dimension accordingly whichever position `-1` is in\n",
    "\n",
    "- Only one -1 is allowed.\n",
    "\n",
    "- The total number of elements must match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(12)\n",
    "b = a.view(3,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WHen to use `.view()` vs `.reshape()`\n",
    "- When you know that the tensor is contiguous, then use `.view()`\n",
    "- If not sure, use `.reshape()`\n",
    "\n",
    "But `.reshape()` is less memory efficient than `.view()` because it creates a new tensor in memory, although its not guaranteed to do so\n",
    "- `.reshape()` creates a copy and allocates new memory when the original tensor is not contiguous. A tensor becomes non-contiguous after certain operations, like `.permute()` or `.transpose()`, which reorder the dimensions without changing the underlying data layout. \n",
    "\n",
    "- If the input tensor is already contiguous, reshape() will simply return a view of the original tensor, just like view() does. In this case, no new memory is allocated, and the operation is just as memory-efficient as view()\n",
    "\n",
    "- `.view()` is more restrictive and will only return a view if the tensor is contiguous. If the tensor is non-contiguous, calling `.view()` will result in a RuntimeError. This forces the user to explicitly handle the contiguity, typically by calling `.contiguous()` first, which performs the memory copy manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "tensor([[0.0801, 0.7271, 0.3827],\n",
      "        [0.2099, 0.8101, 0.2096]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((2,3))\n",
    "print(a.is_contiguous())\n",
    "b = a.T\n",
    "print(b.is_contiguous())\n",
    "\n",
    "\n",
    "a.view(3,-1) # Will work because the tensor is contiguos\n",
    "b.view(2,-1) # runtime error because b is not contiguous\n",
    "\n",
    "print(b.reshape(2,-1)) # Will work even when the tensor is not contiguous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between `.view()` and `.flatten()`\n",
    "\n",
    "` x.view(-1) == x.flatten() `\n",
    "\n",
    "The only difference:\n",
    "- `.flatten()` can flatten across specific dimensions (start_dim, end_dim)\n",
    "- `.view(-1)` flattens the entire tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 210])\n",
      "torch.Size([2520])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((3,4,5,6,7)) # shape = (3,4,5,6,7)\n",
    "b = a.flatten(start_dim=2) # shape = (3,4,210)\n",
    "print(b.shape) # Flattens starting from the start_dim\n",
    "\n",
    "c = a.view(-1) # flattens along all dimensions\n",
    "print(c.shape) # [2520,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When .view() fails silently (common trap)\n",
    "- You may think .view() reshaped the tensor correctly, but if the tensor was non-contiguous, and you used .contiguous() incorrectly, you might accidentally copy data unknowingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "y = x.permute(0, 2, 1)          # non-contiguous\n",
    "z = y.contiguous().view(2, -1)  # makes a copy\n",
    "\n",
    "# Z no longer shares memory with y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mentoroid_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
