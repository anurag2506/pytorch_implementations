{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of the elements in the tensor : 20.046470642089844\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9b/1tm52gk53x99sw1xshhbwcsc0000gn/T/ipykernel_81720/4144037410.py:16: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print(sum.grad)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Matrix multiplication and reduction.\n",
    "Create a random tensor M1 of shape (5, 3) and another random tensor M2 of shape (3, 4).\n",
    "Perform matrix multiplication using the @ operator.\n",
    "Then, take the resulting matrix and compute the sum of all its elements.\n",
    "'''\n",
    "\n",
    "M = torch.rand((5,3),requires_grad=True).to(device)\n",
    "N = torch.rand((3,4)).to(device)\n",
    "\n",
    "P = M @ N\n",
    "sum = torch.sum(P) # by default, it reduces all the dimensions. Else sums along the specific dimension. \n",
    "print(f\"Sum of the elements in the tensor : {sum.item()}\")\n",
    "\n",
    "sum.backward()\n",
    "print(sum.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Finding the maximum values and their indices.\n",
    "Create a tensor with random integer values between 1 and 100, of shape (4, 6). \n",
    "Find the maximum value in the entire tensor. \n",
    "Then, find the maximum value and its corresponding index for each row.\n",
    "'''\n",
    "A = torch.arange(1,100).reshape((4,6)).to(device)\n",
    "max = torch.max(A).item()\n",
    "min = torch.min(A).item()\n",
    "\n",
    "print(f\" Max val of tensor : {max} and the min value is : {min}\")\n",
    "max_per_row = torch.tensor(torch.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Element-wise exponential and square root.\n",
    "Create a tensor x using torch.arange from 0 to 9. Compute the element-wise exponential of x (e^x). \n",
    "Then, compute the square root of the result.\n",
    "'''\n",
    "x = torch.arange(0,9)\n",
    "ex = torch.exp(x)\n",
    "sq_ex = torch.sqrt(ex)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000e+00, 2.7183e+00, 7.3891e+00, 2.0086e+01, 5.4598e+01, 1.4841e+02,\n",
      "        4.0343e+02, 1.0966e+03, 2.9810e+03])\n",
      "tensor([ 1.0000,  1.6487,  2.7183,  4.4817,  7.3891, 12.1825, 20.0855, 33.1155,\n",
      "        54.5981])\n"
     ]
    }
   ],
   "source": [
    "print(ex)\n",
    "print(sq_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Clamping and filtering tensor values.\n",
    "Create a random tensor of shape (5, 5). Clamp all values in the tensor to be between 0.1 and 0.5.\n",
    "'''\n",
    "a = torch.rand((5,5))\n",
    "clamp_a = torch.clamp(a,min=0.1, max=0.5).to(device) # can store the clamped tensor in another tensor\n",
    "clamp_b = a.clamp(0.1,0.5) # both are the same way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.2804, 0.2722, 0.5000, 0.3407],\n",
       "        [0.5000, 0.1494, 0.5000, 0.5000, 0.2805],\n",
       "        [0.4429, 0.5000, 0.5000, 0.1000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.1000, 0.5000, 0.1771],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.3814]], device='mps:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamp_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.2804, 0.2722, 0.5000, 0.3407],\n",
       "        [0.5000, 0.1494, 0.5000, 0.5000, 0.2805],\n",
       "        [0.4429, 0.5000, 0.5000, 0.1000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.1000, 0.5000, 0.1771],\n",
       "        [0.5000, 0.5000, 0.5000, 0.5000, 0.3814]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamp_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
