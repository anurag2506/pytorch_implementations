{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fba3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# --- PyTorch Muscle Memory Challenge ---\n",
    "\n",
    "def linear_forward_pass_and_backward(X: torch.Tensor, Y: torch.Tensor, W: torch.Tensor, B: torch.Tensor) -> tuple[float, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Performs a single layer linear forward pass (Y_pred = X @ W + B), \n",
    "    calculates the Mean Squared Error (MSE) loss, and computes gradients.\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): Input features, shape (N, D_in).\n",
    "        Y (torch.Tensor): Actual target values, shape (N, D_out).\n",
    "        W (torch.Tensor): Weight matrix, shape (D_in, D_out).\n",
    "        B (torch.Tensor): Bias vector, shape (D_out,).\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, torch.Tensor]: The scalar MSE loss, and the calculated \n",
    "                                    gradient of the loss with respect to W (dL/dW).\n",
    "    \"\"\"\n",
    "    # 1. Linear Forward Pass\n",
    "    # Compute the matrix multiplication X @ W and add the bias B.\n",
    "    # Hint: B will be automatically broadcast across the N dimension.\n",
    "    Y_pred = \n",
    "\n",
    "    # 2. Loss Calculation (Mean Squared Error, MSE)\n",
    "    # Calculate the element-wise squared difference, sum it, and divide by the number of elements.\n",
    "    # We want a scalar loss value.\n",
    "    N = X.size(0) # Number of samples\n",
    "    loss = None # <<< YOUR CODE HERE (loss = ...)\n",
    "\n",
    "    # 3. Automatic Differentiation (Backward Pass)\n",
    "    # Compute the gradients of the loss with respect to all tensors that require_grad (W and B).\n",
    "    # You must first zero out any existing gradients (optional but good practice).\n",
    "    # Then, call the .backward() method on the scalar loss.\n",
    "    # <<< YOUR CODE HERE (W.grad.zero_(), loss.backward(), etc.)\n",
    "\n",
    "    # 4. Return results\n",
    "    # Convert the loss tensor to a Python float using .item()\n",
    "    return loss.item(), W.grad\n",
    "\n",
    "\n",
    "# --- Setup and Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Define hyper-parameters and dimensions\n",
    "    N, D_in, D_out = 10, 5, 2  # N=Samples, D_in=Input features, D_out=Output features\n",
    "\n",
    "    # Create dummy input data and target data\n",
    "    X_data = torch.randn(N, D_in)\n",
    "    Y_target = torch.randn(N, D_out)\n",
    "\n",
    "    # Initialize W and B, requiring gradients for optimization\n",
    "    W_param = torch.randn(D_in, D_out, requires_grad=True)\n",
    "    B_param = torch.randn(D_out, requires_grad=True)\n",
    "\n",
    "    print(\"--- PyTorch Challenge Execution ---\")\n",
    "    print(f\"X shape: {X_data.shape}, Y shape: {Y_target.shape}\")\n",
    "    print(f\"W shape: {W_param.shape}, B shape: {B_param.shape}\\n\")\n",
    "\n",
    "    # Run the function\n",
    "    final_loss, grad_W = linear_forward_pass_and_backward(X_data, Y_target, W_param, B_param)\n",
    "\n",
    "    # --- Verification and Output ---\n",
    "    print(f\"1. Final Scalar MSE Loss: {final_loss:.4f}\")\n",
    "    \n",
    "    # Check if gradients were computed\n",
    "    if W_param.grad is not None and B_param.grad is not None:\n",
    "        print(f\"2. Gradient dL/dW computed successfully. Shape: {W_param.grad.shape}\")\n",
    "        print(f\"3. Gradient dL/dB computed successfully. Shape: {B_param.grad.shape}\")\n",
    "        \n",
    "        # Display small sample of the computed gradient\n",
    "        print(f\"   Sample dL/dW (top-left):\\n{W_param.grad[:2, :2]}\")\n",
    "    else:\n",
    "        print(\"Gradients were not successfully computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487e2772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
