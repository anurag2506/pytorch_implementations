{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98c1bede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([2,3,4])\n",
    "\n",
    "torch.sum((a-b)**2)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70fba3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PyTorch Challenge Execution ---\n",
      "X shape: torch.Size([10, 5]), Y shape: torch.Size([10, 2])\n",
      "W shape: torch.Size([5, 2]), B shape: torch.Size([2])\n",
      "\n",
      "1. Final Scalar MSE Loss: 0.0000\n",
      "2. Gradient dL/dW computed successfully. Shape: torch.Size([5, 2])\n",
      "3. Gradient dL/dB computed successfully. Shape: torch.Size([2])\n",
      "   Sample dL/dW (top-left):\n",
      "tensor([[-2.6822, -0.4118],\n",
      "        [ 3.9925, -4.0314]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# --- PyTorch Muscle Memory Challenge ---\n",
    "\n",
    "def linear_forward_pass_and_backward(X: torch.Tensor, Y: torch.Tensor, W: torch.Tensor, B: torch.Tensor) -> tuple[float, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Performs a single layer linear forward pass (Y_pred = X @ W + B), \n",
    "    calculates the Mean Squared Error (MSE) loss, and computes gradients.\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): Input features, shape (N, D_in).\n",
    "        Y (torch.Tensor): Actual target values, shape (N, D_out).\n",
    "        W (torch.Tensor): Weight matrix, shape (D_in, D_out).\n",
    "        B (torch.Tensor): Bias vector, shape (D_out,).\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, torch.Tensor]: The scalar MSE loss, and the calculated \n",
    "                                    gradient of the loss with respect to W (dL/dW).\n",
    "    \"\"\"\n",
    "    # 1. Linear Forward Pass\n",
    "    # Compute the matrix multiplication X @ W and add the bias B.\n",
    "    # Hint: B will be automatically broadcast across the N dimension.\n",
    "    Y_pred = X @ W + B\n",
    "\n",
    "    # 2. Loss Calculation (Mean Squared Error, MSE)\n",
    "    # Calculate the element-wise squared difference, sum it, and divide by the number of elements.\n",
    "    # We want a scalar loss value.\n",
    "    N = X.size(0) # Number of samples\n",
    "    loss = torch.sum((Y_pred - Y)**2)/N\n",
    "\n",
    "    # 3. Automatic Differentiation (Backward Pass)\n",
    "    # Compute the gradients of the loss with respect to all tensors that require_grad (W and B).\n",
    "    # You must first zero out any existing gradients (optional but good practice).\n",
    "    # Then, call the .backward() method on the scalar loss.\n",
    "    # loss.requires_grad()\n",
    "    loss.backward()\n",
    "    loss.zero_()\n",
    "\n",
    "    # 4. Return results\n",
    "    # Convert the loss tensor to a Python float using .item()\n",
    "    return loss.item(), W.grad\n",
    "\n",
    "\n",
    "# --- Setup and Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Define hyper-parameters and dimensions\n",
    "    N, D_in, D_out = 10, 5, 2  # N=Samples, D_in=Input features, D_out=Output features\n",
    "\n",
    "    # Create dummy input data and target data\n",
    "    X_data = torch.randn(N, D_in)\n",
    "    Y_target = torch.randn(N, D_out)\n",
    "\n",
    "    # Initialize W and B, requiring gradients for optimization\n",
    "    W_param = torch.randn(D_in, D_out, requires_grad=True)\n",
    "    B_param = torch.randn(D_out, requires_grad=True)\n",
    "\n",
    "    print(\"--- PyTorch Challenge Execution ---\")\n",
    "    print(f\"X shape: {X_data.shape}, Y shape: {Y_target.shape}\")\n",
    "    print(f\"W shape: {W_param.shape}, B shape: {B_param.shape}\\n\")\n",
    "\n",
    "    # Run the function\n",
    "    final_loss, grad_W = linear_forward_pass_and_backward(X_data, Y_target, W_param, B_param)\n",
    "\n",
    "    # --- Verification and Output ---\n",
    "    print(f\"1. Final Scalar MSE Loss: {final_loss:.4f}\")\n",
    "    \n",
    "    # Check if gradients were computed\n",
    "    if W_param.grad is not None and B_param.grad is not None:\n",
    "        print(f\"2. Gradient dL/dW computed successfully. Shape: {W_param.grad.shape}\")\n",
    "        print(f\"3. Gradient dL/dB computed successfully. Shape: {B_param.grad.shape}\")\n",
    "        \n",
    "        # Display small sample of the computed gradient\n",
    "        print(f\"   Sample dL/dW (top-left):\\n{W_param.grad[:2, :2]}\")\n",
    "    else:\n",
    "        print(\"Gradients were not successfully computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487e2772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5450afa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
