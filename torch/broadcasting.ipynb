{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2390, 0.8470, 0.6567, 0.6765],\n",
      "        [1.7383, 1.3463, 1.1559, 1.1758],\n",
      "        [0.8531, 0.4611, 0.2707, 0.2906]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Basic arithmetic with broadcasting.\n",
    "Create a tensor A of shape (3, 1) and a tensor B of shape (1, 4). \n",
    "Perform element-wise addition and element-wise multiplication on A and B. \n",
    "Explain how broadcasting allows this operation to be performed.\n",
    "'''\n",
    "\n",
    "A = torch.rand(size=(3,1)).to(device)\n",
    "B = torch.rand(size=(1,4)).to(device)\n",
    "\n",
    "n = A.shape[0]\n",
    "m = B.shape[1]\n",
    "\n",
    "C = torch.zeros(size=(n,m)).to(device)\n",
    "for i in range(n):\n",
    "    for j in range(m):\n",
    "        C[i][j] = A[i][0] + B[0][j]\n",
    "\n",
    "print(C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_= torch.broadcast_shapes((3,1),(1,4)) # C gets broadcasted shape of (3,4)\n",
    "C_ = A+B\n",
    "assert(torch.sum(C_ == C) == C.shape[0]*C.shape[1]) \n",
    "# Number of equal elements in both the tensors should be equal to the tot number of tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting:\n",
    "- In linear algebra, addition, subtraction, multiplication, and division of matrices (or tensors) require them to have the exact same shape (element-wise operations).\n",
    "- Broadcasting is a mechanism that relaxes this constraint. When two tensors have different shapes, PyTorch (and NumPy) attempts to align them by \"stretching\" the smaller tensor along its dimension(s) so that the resulting shapes are compatible. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule of Broadcasting:\n",
    "- Each tensor has at least one dimension.\n",
    "- When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n",
    "- If a dimension is missing, always pad in the leading dimensions. Eg. (4,) -> (1,4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eg. A = (3,1,2), B = (2,)\n",
    "These 2 are broadcastable because:\n",
    "\n",
    "```\n",
    "A = (3,1,2)\n",
    "B = (1,1,2)\n",
    "```\n",
    "\n",
    "And hence the result of the broadcasted tensor is (3,1,2)\n",
    "\n",
    "Eg. \n",
    "\n",
    "```\n",
    "A = (4, 1, 6, 1)\n",
    "B = (1, 5, 1, 8)\n",
    "```\n",
    "\n",
    "C = A + B <br>\n",
    "C = (4,5,6,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function `is_broadcastable(shape1, shape2)` \n",
    "# that returns True/False if the two shapes can be broadcast together\n",
    "# according to PyTorch rules.\n",
    "#\n",
    "# Example:\n",
    "# is_broadcastable((4,1,3), (1,5,1)) ➜ True\n",
    "# is_broadcastable((4,2), (3,)) ➜ False\n",
    "\n",
    "def is_broadcastable(A: torch.Tensor, B:torch.tensor)-> bool:\n",
    "    shape1 = A.shape\n",
    "    shape2 = B.shape\n",
    "\n",
    "    if len(shape1) > len(shape2):\n",
    "        while(len(shape1) != len(shape2)):\n",
    "            B = torch.unsqueeze(B, dim=0)\n",
    "            shape2 = B.shape\n",
    "    else:\n",
    "        while(len(shape1) != len(shape2)):\n",
    "            A = torch.unsqueeze(A, dim=0)\n",
    "            shape1 = A.shape\n",
    "    \n",
    "    assert(len(shape1) == len(shape2))\n",
    "    print(f\"Shape of A : {A.shape} \\nShape of B : {B.shape}\")\n",
    "    \n",
    "    n = len(shape1)\n",
    "    \n",
    "    for i in range(n):\n",
    "        if ((shape1[n-i-1] != shape2[n-i-2]) and (shape1[n-i-1] != 1) and (shape2[n-i-1] != 1)):\n",
    "            return False\n",
    "    \n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A : torch.Size([4, 1, 3]) \n",
      "Shape of B : torch.Size([1, 5, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.rand(size=(4,1,3))\n",
    "B = torch.rand(size=(1,5,1))\n",
    "is_broadcastable(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4631818272\n",
      "5144374112\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor of shape (3,1)\n",
    "# Expand it to (3,5) using .expand()\n",
    "# Verify that .expand() doesn’t allocate new memory (use a.is_shared_storage(b))\n",
    "\n",
    "a = torch.rand(size=(3,1)).to(\"mps\")\n",
    "b = torch.expand_copy(a,size=(3,5)).to(\"mps\")\n",
    "print(a.data_ptr())\n",
    "print(b.data_ptr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "# Given x = torch.rand(3,1,5) and y = torch.rand(1,4,1)\n",
    "# Perform x + y and print shape.\n",
    "# Confirm with manual reasoning.\n",
    "x = torch.rand(3,1,5)\n",
    "y = torch.rand(1,4,1)\n",
    "\n",
    "z = x+y\n",
    "print(z.shape)\n",
    "z_ = torch.broadcast_shapes(x.shape,y.shape)\n",
    "# assert(z.shape == z_.shape)\n",
    "assert(z.shape == z_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalised Data :tensor([[-1.4327e+00,  1.5524e-01, -6.3545e-01,  2.5937e-01,  9.4289e-01,\n",
      "         -6.0890e-01,  3.8253e-01,  1.1252e+00,  4.2250e-01,  7.5454e-04],\n",
      "        [-5.5106e-01,  1.5899e+00,  8.7424e-01, -1.0110e+00,  1.1499e+00,\n",
      "          1.5305e+00,  1.2008e+00,  4.8330e-01, -6.6470e-01, -1.0054e+00],\n",
      "        [ 1.0955e+00, -5.7064e-01, -1.3766e+00, -8.7428e-01, -2.5748e-01,\n",
      "         -1.1187e+00, -1.4135e+00, -1.4218e+00, -1.2974e+00,  3.1168e-01],\n",
      "        [ 2.8755e-01, -1.0524e+00,  1.8848e-01,  1.4550e+00, -7.7960e-01,\n",
      "          1.8778e-01,  3.6193e-01, -5.7360e-01,  1.2754e+00, -8.0501e-01],\n",
      "        [ 6.0072e-01, -1.2217e-01,  9.4936e-01,  1.7089e-01, -1.0557e+00,\n",
      "          9.2866e-03, -5.3169e-01,  3.8696e-01,  2.6426e-01,  1.4980e+00]])\n"
     ]
    }
   ],
   "source": [
    "# Given a tensor X of shape (batch_size, num_features)\n",
    "# Subtract mean of each feature and divide by its std, \n",
    "# using broadcasting (without explicit loops).\n",
    "# X_norm = (X - X.mean(dim=0)) / X.std(dim=0)\n",
    "\n",
    "batch_size = 5\n",
    "num_features = 10\n",
    "X = torch.rand(size=(batch_size,num_features))\n",
    "mean = torch.mean(X,dim=0) # mean along every feature and therefore there will be 10 different means\n",
    "std = torch.std(X,dim=0)\n",
    "X = (X-mean)/std\n",
    "\n",
    "print(f\"Normalised Data :{X}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (30) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m B = torch.rand(size=(M,D))\n\u001b[32m     11\u001b[39m DIST = torch.zeros(size=(N,M))\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m DIST = (\u001b[43mA\u001b[49m\u001b[43m-\u001b[49m\u001b[43mB\u001b[49m)**\u001b[32m2\u001b[39m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (10) must match the size of tensor b (30) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "# Given two tensors A and B of shapes (N, D) and (M, D),\n",
    "# compute the pairwise Euclidean distance matrix of shape (N, M)\n",
    "# using broadcasting (no loops).\n",
    "#\n",
    "N = 10\n",
    "D = 20\n",
    "M = 30\n",
    "\n",
    "A = torch.rand(size=(N,D))\n",
    "B = torch.rand(size=(M,D))\n",
    "DIST = torch.zeros(size=(N,M))\n",
    "\n",
    "DIST = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 1])\n",
      "torch.Size([32, 3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "b = torch.rand((3,)).to(device)\n",
    "X = torch.rand((32,3,28,28)).to(device)\n",
    "b = b.unsqueeze(1).unsqueeze(1)\n",
    "print(b.shape)\n",
    "\n",
    "# c = torch.broadcast_shapes(b.shape,X.shape)\n",
    "y = X+b\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 28, 28])\n",
      "torch.Size([1, 3, 1, 1])\n",
      "torch.Size([32, 3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Given an input tensor x of shape (batch, channels, height, width)\n",
    "# and a bias tensor b of shape (channels,),\n",
    "# add the bias to each channel using broadcasting.\n",
    "\n",
    "X = torch.rand((32,3,28,28)).to(device)\n",
    "\n",
    "# One method of doing this:\n",
    "'''\n",
    "b = torch.rand((3,)).to(device)\n",
    "\n",
    "b = b.unsqueeze(1)\n",
    "b = b.unsqueeze(2).unsqueeze(2)\n",
    "'''\n",
    "\n",
    "# Other method of doing this:\n",
    "'''\n",
    "b = b.unsqueeze(1).unsqueeze(1)\n",
    "print(b.shape)\n",
    "'''\n",
    "\n",
    "y = X+b\n",
    "print(X.shape)\n",
    "print(b.shape)\n",
    "print(y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "inconsistent tensor size, expected tensor [4] and src [3] to have the same number of elements, but got 4 and 3 elements respectively",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m a = torch.rand((\u001b[32m4\u001b[39m,))\n\u001b[32m      4\u001b[39m b = torch.rand((\u001b[32m3\u001b[39m,))\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m dot_ = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: inconsistent tensor size, expected tensor [4] and src [3] to have the same number of elements, but got 4 and 3 elements respectively"
     ]
    }
   ],
   "source": [
    "# Given vectors a (shape (3,)) and b (shape (4,)), \n",
    "# compute their outer product (3x4) using broadcasting.\n",
    "a = torch.rand((4,))\n",
    "b = torch.rand((3,))\n",
    "dot_ = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a function that normalizes tensor x \n",
    "# such that softmax is applied over the last dimension:\n",
    "# def softmax(x):\n",
    "#     exp_x = torch.exp(x - x.max(dim=-1, keepdim=True).values)\n",
    "#     return exp_x / exp_x.sum(dim=-1, keepdim=True)\n",
    "#\n",
    "# Understand how broadcasting ensures correct denominator division.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given A, B of shape (batch, dim), compute cosine similarity per batch:\n",
    "# cosine_sim = (A * B).sum(dim=1) / (A.norm(dim=1) * B.norm(dim=1))\n",
    "# Identify all broadcasted operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an image batch x of shape (N, C, H, W)\n",
    "# Normalize each channel using channel-wise mean and std (both of shape (C,))\n",
    "# Use broadcasting to implement this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given logits of shape (batch, num_classes)\n",
    "# and labels as class indices of shape (batch,)\n",
    "# implement one-hot encoding manually and compute MSE loss using broadcasting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor t of shape (10,)\n",
    "# Reshape and expand it to (10, 5, 1, 8) using view/unsqueeze/expand combo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.rand(5,4,3)\n",
    "# y = torch.rand(4,1)\n",
    "# x + y  # Throws error.\n",
    "# Fix it using view/unsqueeze so broadcasting works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor image (1, 1, 5, 5) and kernel (1, 1, 3, 3)\n",
    "# Use unfold() + broadcasting to manually perform 2D convolution\n",
    "# (no nn.Conv2d, pure tensor ops).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement your own function broadcast_add(a, b) that adds two tensors manually by:\n",
    "# Expanding singleton dimensions yourself\n",
    "# Using .expand() or .repeat()\n",
    "# Without using a + b directly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
