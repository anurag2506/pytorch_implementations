{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2390, 0.8470, 0.6567, 0.6765],\n",
      "        [1.7383, 1.3463, 1.1559, 1.1758],\n",
      "        [0.8531, 0.4611, 0.2707, 0.2906]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Basic arithmetic with broadcasting.\n",
    "Create a tensor A of shape (3, 1) and a tensor B of shape (1, 4). \n",
    "Perform element-wise addition and element-wise multiplication on A and B. \n",
    "Explain how broadcasting allows this operation to be performed.\n",
    "'''\n",
    "\n",
    "A = torch.rand(size=(3,1)).to(device)\n",
    "B = torch.rand(size=(1,4)).to(device)\n",
    "\n",
    "n = A.shape[0]\n",
    "m = B.shape[1]\n",
    "\n",
    "C = torch.zeros(size=(n,m)).to(device)\n",
    "for i in range(n):\n",
    "    for j in range(m):\n",
    "        C[i][j] = A[i][0] + B[0][j]\n",
    "\n",
    "print(C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_= torch.broadcast_shapes((3,1),(1,4)) # C gets broadcasted shape of (3,4)\n",
    "C_ = A+B\n",
    "assert(torch.sum(C_ == C) == C.shape[0]*C.shape[1]) \n",
    "# Number of equal elements in both the tensors should be equal to the tot number of tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting:\n",
    "- In linear algebra, addition, subtraction, multiplication, and division of matrices (or tensors) require them to have the exact same shape (element-wise operations).\n",
    "- Broadcasting is a mechanism that relaxes this constraint. When two tensors have different shapes, PyTorch (and NumPy) attempts to align them by \"stretching\" the smaller tensor along its dimension(s) so that the resulting shapes are compatible. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule of Broadcasting:\n",
    "- Each tensor has at least one dimension.\n",
    "- When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n",
    "- If a dimension is missing, always pad in the leading dimensions. Eg. (4,) -> (1,4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eg. A = (3,1,2), B = (2,)\n",
    "These 2 are broadcastable because:\n",
    "\n",
    "```\n",
    "A = (3,1,2)\n",
    "B = (1,1,2)\n",
    "```\n",
    "\n",
    "And hence the result of the broadcasted tensor is (3,1,2)\n",
    "\n",
    "Eg. \n",
    "\n",
    "```\n",
    "A = (4, 1, 6, 1)\n",
    "B = (1, 5, 1, 8)\n",
    "```\n",
    "\n",
    "C = A + B <br>\n",
    "C = (4,5,6,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function `is_broadcastable(shape1, shape2)` \n",
    "# that returns True/False if the two shapes can be broadcast together\n",
    "# according to PyTorch rules.\n",
    "#\n",
    "# Example:\n",
    "# is_broadcastable((4,1,3), (1,5,1)) ➜ True\n",
    "# is_broadcastable((4,2), (3,)) ➜ False\n",
    "\n",
    "def is_broadcastable(A: torch.Tensor, B:torch.tensor)-> bool:\n",
    "    shape1 = A.shape\n",
    "    shape2 = B.shape\n",
    "\n",
    "    if len(shape1) > len(shape2):\n",
    "        while(len(shape1) != len(shape2)):\n",
    "            B = torch.unsqueeze(B, dim=0)\n",
    "            shape2 = B.shape\n",
    "    else:\n",
    "        while(len(shape1) != len(shape2)):\n",
    "            A = torch.unsqueeze(A, dim=0)\n",
    "            shape1 = A.shape\n",
    "    \n",
    "    assert(len(shape1) == len(shape2))\n",
    "    print(f\"Shape of A : {A.shape} \\nShape of B : {B.shape}\")\n",
    "    \n",
    "    n = len(shape1)\n",
    "    \n",
    "    for i in range(n):\n",
    "        if ((shape1[n-i-1] != shape2[n-i-2]) and (shape1[n-i-1] != 1) and (shape2[n-i-1] != 1)):\n",
    "            return False\n",
    "    \n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A : torch.Size([4, 1, 3]) \n",
      "Shape of B : torch.Size([1, 5, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.rand(size=(4,1,3))\n",
    "B = torch.rand(size=(1,5,1))\n",
    "is_broadcastable(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5485646976\n",
      "5485955584\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor of shape (3,1)\n",
    "# Expand it to (3,5) using .expand()\n",
    "# Verify that .expand() doesn’t allocate new memory (use a.is_shared_storage(b))\n",
    "\n",
    "a = torch.rand(size=(3,1))\n",
    "b = torch.expand_copy(a,size=(3,5))\n",
    "print(a.data_ptr())\n",
    "print(b.data_ptr())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
